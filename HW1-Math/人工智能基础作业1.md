2400010833 闵诗珈

# 人工智能基础

### 作业一



1. 请简述什么是贝叶斯定理，什么是最大似然估计（MLE），什么是最大后验估计（MAP）。

   1. 贝叶斯定理：$p(Y|X)=\frac{p(X|Y)p(Y)}{\sum_Yp(X|Y)p(Y)}$，即**后验概率正比于似然和先验**。它描述了两个条件概率之间的关系，可在已知一些事件可能与某事件相关前提下，用于预测某事件的条件概率。

   2. MLE：找到使得观测数据集出现概率最大的参数。

      (便于计算，防止频率计算下溢，因此常取log)
      $$
      \theta_{MLE}=arg\underset{\theta}{\max}p(X|\theta)=arg\underset{\theta}{\max}\sum_{i=1}^{N}p(x_i|\theta)
      $$

   3. MAP：在找参数时不仅考虑观测数据，还考虑对参数的先验知识。
      $$
      \theta_{MAP}=arg\underset{\theta}{\max}p(\theta|X)=\underset{\theta}{\max}p(X|\theta)p(\theta)
      $$


   

2. 设$X\sim N(\mu, \sigma^2)$，$\mu, \sigma^2$为未知参数，$x_1,x_2,... , x_n$是来自$X$的样本值，求$\mu, \sigma^2$的最大似然估计量。

   正态分布的概率密度函数是
   $$
   f(x;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
   $$
   要最大化给定数据的参数的概率
   $$
   p(\mathbf{x}|\mu,\sigma^2)=\prod_{n = 1}^{N}\mathcal{N}(x_n|\mu,\sigma^2)
   $$
   即要最大化似然函数的对数
   $$
   \ln p(\mathbf{x}|\mu,\sigma^2)=\sum_{n = 1}^{N}(x_n-\mu)^2-\frac{N}{2}\ln \sigma^2-\frac{N}{2} \ln 2\pi
   $$
   对$\mu$求偏导 
   $$
   \frac{\partial}{\partial\mu} \ln p(x|\mu, \sigma^{2}) = \sum_{n = 1}^{N} \left[-\frac{1}{2\sigma^{2}} \cdot 2(x_{n} - \mu) \cdot (-1)\right] = \frac{1}{\sigma^{2}}\sum_{n = 1}^{N}(x_{n} - \mu)
   $$
   对$\sigma^{2}$求偏导 
   $$
   \begin{align*} \frac{\partial}{\partial\sigma^{2}} \left[-\frac{1}{2\sigma^{2}}\sum_{n = 1}^{N}(x_{n} - \mu)^{2}\right] &= \frac{1}{2(\sigma^{4})}\sum_{n = 1}^{N}(x_{n} - \mu)^{2}\\ \frac{\partial}{\partial\sigma^{2}} \left[-\frac{N}{2} \ln \sigma^{2}\right] &= -\frac{N}{2} \cdot \frac{1}{\sigma^{2}} \\
   \Rightarrow \frac{\partial}{\partial\mu} \ln p(x|\mu, \sigma^{2}) &= \frac{1}{2(\sigma^{4})}\sum_{n = 1}^{N}(x_{n} - \mu)^{2}--\frac{N}{2} \cdot \frac{1}{\sigma^{2}}
   \end{align*}
   $$
   令上两式等于0，可解得
   $$
   \begin{align*}\mu_{ML}&=\frac{1}{N}
   \sum_{n=1}^{N}x_n\\
   \sigma_{ML}^2 &=\frac{1}{N}\sum_{n=1}^{N}(x_n-\mu_{ML})^2
   \end{align*}
   $$
   

3. 请简述分类问题与回归问题的主要区别。

   |          |                         分类问题                         |                        回归问题                        |
   | :------: | :------------------------------------------------------: | :----------------------------------------------------: |
   |   目标   |                     确定样本所属类别                     |                给出输入数据的一个映射值                |
   | 输出类型 |                          离散的                          |                         连续的                         |
   | 常用算法 | 线性回归、岭回归、Lasso 回归、决策树回归、随机森林回归等 | 决策树、支持向量机、朴素贝叶斯、K 近邻算法、逻辑回归等 |
   | 评估标准 |           准确率、精确率、召回率、F1 值等指标            |     根据需要使用误差度量指标，如$R^2$、MSE、MAE等      |

   

4. 请简述有监督学习与无监督学习的主要区别。

   |          |                           监督学习                           |                无监督学习                |
   | :------: | :----------------------------------------------------------: | :--------------------------------------: |
   | 输入数据 | 包含了一系列问题与正确答案的数据集{$x_n, y_n$} ($n=1,2,...,N$) | 没有答案或者标签{$x_n$}  ($n=1,2,...,N$) |
   |   目标   |                       预测给定x对应的y                       |         发现数据内在的规律和结构         |
   |   任务   |                          回归、分类                          |           聚类、降维、密度估计           |

   

5. 给定数据 $D = \{(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\}$，用一个线性模型估计最接近真实$y_i$（ground truth）的连续标量$Y$，$f(x_i) = w^T x_i + b$，such that $f(x_i) \approx y_i$。求最优使得与之间的均方误差最小：
   $$
   (w^*, b^*) = \underset{(w, b)}{\arg\min}\sum_{i = 1}^{n}(f(x_i) - y_i)^2
   $$
   并解释$(w^*, b^*)$何时有 closed form 解，何时没有 closed form 解。

   1. **Least Squares Estimator: **

      我们假设每个样本特征有p个维度（其中为了方便实现矩阵表达，最后一个维度是常数1）

      $$
      \hat{\beta}=arg \underset{(w, b)}{min}\frac{1}{n}(\mathbf{A}\beta- \mathbf{Y})^{\mathrm{T}}(\mathbf{A}\beta - \mathbf{Y})
      $$
      （其中A是一个n\*p矩阵，每个数据一行；Y是一个n\*1矩阵，与A对应的每个标签一行）

      令 $J(\beta)=(\mathbf{A}\beta- \mathbf{Y})^{\mathrm{T}}(\mathbf{A}\beta - \mathbf{Y}) $

      求偏导数并令其为0

      $$
      \left.\frac{\partial J(\beta)}{\partial \beta}\right|_{\hat{\beta}} = 2\mathbf{A}^{T}\mathbf{A}\beta - 2\mathbf{A}^{T}\mathbf{Y} = 0
      $$
      得到方程

      $$
      (\mathbf{A}^T\mathbf{A})\hat\beta=\mathbf{A}^T\mathbf{Y}
      $$

   2. 当$(\mathbf{A}^T\mathbf{A})$可逆（满秩）的时候有 closed form 解。

      其不可逆的时候则没有 closed form 解。特别的，如果样本数量**n**<特征维度**p**，则一定不可逆。

      

6. Ridge regression 问题的解具有什么特点，为什么？Lasso 问题的解具有什么特点？为什么？

   1. Ridge regression 的解**有些w更小**，即解具有稳定性、更加平滑，可以降低方差、提高泛化能力；但不能剔除变量。

      原因是选择高斯分布作为先验分布，在误差函数正则化中引入了$L_2$范数惩罚项，$\lambda \sum_{j=1}^p \beta_j^2$，会对大的系数施加更大的惩罚。

   2. Lasso regression导致稀疏的解（**非零w更少**，可以达到剔除部分变量、实现特征选择的目的）

      原因是使用了拉普拉斯分布作为先验分布，引入 $L_1$范数惩罚项，$\lambda \sum_{j=1}^p |\beta_j|$

   

7. 请从 model function、loss function、optimization solution 三个方面比较 Linear regression 与 Logistic regression 的异同。

   |                       | 同                 | Linear regression                          | Logistic regression                                          |
   | --------------------- | ------------------ | ------------------------------------------ | ------------------------------------------------------------ |
   | model function        |                    | $f(\mathbf{x})=w^T\mathbf{x}$ 输出：任何值 | $f(\mathbf{x})=\sigma(w^T\mathbf{x}) , 其中\sigma(a)=\frac{1}{1+e^{-a}}$ 输出：0和1之间 |
   | loss function         |                    | 均方差                                     | 交叉熵：预测概率分布之间的差异                               |
   | optimization solution | 可用梯度下降法求解 | 或最小二乘法                               |                                                              |

   

8. K - 近邻分类器的超参数是什么？怎么选择 K - 近邻分类器的超参数？

    1. 超参数：
    - 选择最近的数据点（K）的数量
    - 用何种距离度量（Manhattan, Euclidean）
    2. 怎么选择：

    - 尝试所有的取值；将数据集分为训练集、验证集和测试集，使用训练集预测，在验证集上选择超参数，在测试集上运行一次进行测试。

